version: "3.9"

services:
  nllb-serve:
    build: .
    ports:
      - "8123:6060"
    environment:
      - MODEL=facebook/nllb-200-distilled-600M
      - DEVICE=${DEVICE:-auto}       # "cuda" nutzt GPU, "cpu" erzwingt CPU, "auto" versucht GPU -> CPU fallback
      - CT2_USE_FP16=1               # Schnellere Inferenz mit halber Genauigkeit (nur relevant bei GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    volumes:
      - ./models:/root/.cache/huggingface
    restart: unless-stopped
    command: ["python", "-m", "nllb_serve", "--model", "facebook/nllb-200-distilled-600M", "--port", "6060"]
